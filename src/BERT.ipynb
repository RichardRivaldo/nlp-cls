{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sxw6aqBp0Pjm"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1UPjSICxNoP",
        "outputId": "ecbb4a17-0310-4e8a-b1c9-80bc78e2e487"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqJ9--cx0Xkg"
      },
      "outputs": [],
      "source": [
        "root_path = \"drive/MyDrive/NLP/Text Classification/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fVHQyul4UJH",
        "outputId": "b775cb1d-b46f-4228-b5a9-4472b5729269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Sastrawi in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.22.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "! pip install Sastrawi\n",
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrGY9Pqo4B9t"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADnhmMHJ0mCP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from tqdm import trange\n",
        "from transformers import BertTokenizer, BertForSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzEd5dMY4GRl"
      },
      "source": [
        "# Data + Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oN8LSJT4Jmk"
      },
      "source": [
        "## Reading the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9x9gAJHiDo2m"
      },
      "outputs": [],
      "source": [
        "def tokenize(sent):\n",
        "    tokens = sent.split()\n",
        "    tokens = list(filter(lambda token: len(token) > 1, tokens))\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahk9ku-N4NU1"
      },
      "outputs": [],
      "source": [
        "# Split label and features\n",
        "def split_dataframe(df):\n",
        "    df_features = df.loc[:, \"text_a\"].str.lower()\n",
        "\n",
        "    factory = StopWordRemoverFactory()\n",
        "    stopword = factory.create_stop_word_remover()\n",
        "\n",
        "    df_features = df_features.apply(lambda x: stopword.remove(x))\n",
        "\n",
        "    return df_features, df.loc[:, \"label\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xZOrZNY4as7"
      },
      "source": [
        "### Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XM8HuIte4eKQ"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(f\"{root_path}train.csv\")\n",
        "df_train.drop(\"Unnamed: 0\", axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2kKKWn14zrz"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = split_dataframe(df_train)\n",
        "# X_train, y_train = X_train[:100], y_train[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cIXJVpb46tz"
      },
      "source": [
        "### Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZdBf4Pz46t0"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv(f\"{root_path}test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NljiT9vO46t0"
      },
      "outputs": [],
      "source": [
        "X_test, y_test = split_dataframe(df_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMMiWHcEII3o"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Vnau-boIiJ-",
        "outputId": "f80c5d70-ca84-485b-b789-1dfd95997d96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15.61811953150317"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Mencari panjang rata-rata dari kalimat yang ada\n",
        "df_train[\"text_a\"].apply(lambda x: len(x.split())).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POHbNUrZDwD-"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qex39bX7eJxv"
      },
      "source": [
        "## Labelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdMFQvs_eMH0"
      },
      "outputs": [],
      "source": [
        "label_to_code = {'no': 0, 'yes': 1}\n",
        "\n",
        "y_train = y_train.apply(lambda x : label_to_code[x])\n",
        "y_test = y_test.apply(lambda x : label_to_code[x])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7rFMuJeLKss"
      },
      "source": [
        "## Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(train_data, test_data, max_length):\n",
        "  tokenizer = BertTokenizer.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    do_lower_case = True\n",
        "  )\n",
        "\n",
        "  # Train Data\n",
        "  input_ids_train = []\n",
        "  attention_mask_train = []\n",
        "\n",
        "  for text in train_data:\n",
        "    encoded = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens = True,\n",
        "        max_length = max_length,\n",
        "        padding = 'max_length',\n",
        "        truncation = True,\n",
        "        return_attention_mask = True,\n",
        "        return_tensors = 'pt'\n",
        "    )\n",
        "    input_ids_train.append(encoded['input_ids'])\n",
        "    attention_mask_train.append(encoded['attention_mask'])\n",
        "\n",
        "  input_ids_train = torch.cat(input_ids_train, dim = 0)\n",
        "  attention_mask_train = torch.cat(attention_mask_train, dim = 0)\n",
        "  y_tf_train = torch.tensor(y_train)\n",
        "\n",
        "  # Test Data\n",
        "  input_ids_test = []\n",
        "  attention_mask_test = []\n",
        "\n",
        "  for text in test_data:\n",
        "    encoded = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens = True,\n",
        "        max_length = max_length,\n",
        "        padding = 'max_length',\n",
        "        truncation = True,\n",
        "        return_attention_mask = True,\n",
        "        return_tensors = 'pt'\n",
        "    )\n",
        "    input_ids_test.append(encoded['input_ids'])\n",
        "    attention_mask_test.append(encoded['attention_mask'])\n",
        "\n",
        "  input_ids_test = torch.cat(input_ids_test, dim = 0)\n",
        "  attention_mask_test = torch.cat(attention_mask_test, dim = 0)\n",
        "  y_tf_test = torch.tensor(y_test)\n",
        "\n",
        "  return input_ids_train, attention_mask_train, y_tf_train, input_ids_test, attention_mask_test, y_tf_test"
      ],
      "metadata": {
        "id": "ZldV2TE0cU_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESuLhQvvgP96"
      },
      "source": [
        "## Data Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AG2VjbLYgRr_"
      },
      "outputs": [],
      "source": [
        "def data_load(input_ids_train, attention_mask_train, y_tf_train, input_ids_test, attention_mask_test, y_tf_test) :\n",
        "  train_set = TensorDataset(input_ids_train, attention_mask_train, y_tf_train)\n",
        "  test_set = TensorDataset(input_ids_test, attention_mask_test, y_tf_test)\n",
        "\n",
        "  # Set up dataloader\n",
        "  train_dl = DataLoader(\n",
        "      train_set,\n",
        "      sampler = RandomSampler(train_set),\n",
        "      batch_size = 32\n",
        "  )\n",
        "\n",
        "  test_dl = DataLoader(\n",
        "      test_set,\n",
        "      sampler = SequentialSampler(test_set),\n",
        "      batch_size = 32\n",
        "  )\n",
        "\n",
        "  return train_dl, test_dl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrNQuI1bf21F"
      },
      "source": [
        "## Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wg9R_PqRf5rq"
      },
      "outputs": [],
      "source": [
        "def calculate_tp(preds, labels):\n",
        "  return sum([preds == labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def calculate_fp(preds, labels):\n",
        "  return sum([preds != labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def calculate_tn(preds, labels):\n",
        "  return sum([preds == labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def calculate_fn(preds, labels):\n",
        "  return sum([preds != labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def metrics(preds, labels):\n",
        "  '''\n",
        "  Returns the following metrics:\n",
        "    - accuracy    = (TP + TN) / N\n",
        "    - precision   = TP / (TP + FP)\n",
        "    - recall      = TP / (TP + FN)\n",
        "  '''\n",
        "  preds = np.argmax(preds, axis = 1).flatten()\n",
        "  labels = labels.flatten()\n",
        "  tp = calculate_tp(preds, labels)\n",
        "  tn = calculate_tn(preds, labels)\n",
        "  fp = calculate_fp(preds, labels)\n",
        "  fn = calculate_fn(preds, labels)\n",
        "  accuracy = (tp + tn) / len(labels)\n",
        "  precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n",
        "  recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n",
        "  return accuracy, precision, recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99WJYvtd7nlp"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoBykC787pFa"
      },
      "outputs": [],
      "source": [
        "class bert_model :\n",
        "  def __init__(self, learning_rate, epochs):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.epochs = epochs\n",
        "    \n",
        "    self.model_name = 'bert-base-uncased'\n",
        "\n",
        "    self.model = BertForSequenceClassification.from_pretrained(\n",
        "        self.model_name,\n",
        "        num_labels = 2,\n",
        "        output_attentions = False,\n",
        "        output_hidden_states = False,\n",
        "    )\n",
        "\n",
        "    self.optimizer = torch.optim.AdamW(self.model.parameters(), lr = self.learning_rate)\n",
        "\n",
        "    self.model.cuda()\n",
        "\n",
        "  def train(self, train_dl, test_dl):\n",
        "    for _ in trange(self.epochs, desc = 'Epoch'):\n",
        "      # Training process\n",
        "      # Set to train the model\n",
        "      self.model.train()\n",
        "\n",
        "      loss = 0\n",
        "      steps = 0\n",
        "      for step, batch in enumerate(train_dl):\n",
        "        batch_input_ids, batch_input_mask, batch_labels = tuple(t.to('cuda') for t in batch)\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Forward\n",
        "        forward_out = self.model(batch_input_ids, \n",
        "                            token_type_ids = None, \n",
        "                            attention_mask = batch_input_mask, \n",
        "                            labels = batch_labels)\n",
        "\n",
        "        # Backward\n",
        "        forward_out.loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Loss\n",
        "        loss += forward_out.loss.item()\n",
        "        steps += 1\n",
        "\n",
        "      # Validation process\n",
        "      # Set to validate the model\n",
        "      self.model.eval()\n",
        "\n",
        "      accuracy = []\n",
        "      precision = []\n",
        "      recall = []\n",
        "\n",
        "      for batch in test_dl:\n",
        "        batch_input_ids, batch_input_mask, batch_labels = tuple(t.to('cuda') for t in batch)\n",
        "        with torch.no_grad():\n",
        "          # Forward\n",
        "          eval_out = self.model(batch_input_ids, \n",
        "                            token_type_ids = None, \n",
        "                            attention_mask = batch_input_mask)\n",
        "        logits = eval_out.logits.detach().cpu().numpy()\n",
        "        label_ids = batch_labels.to('cpu').numpy()\n",
        "\n",
        "        # Metrics\n",
        "        batch_accuracy, batch_precision, batch_recall = metrics(logits, label_ids)\n",
        "\n",
        "        accuracy.append(batch_accuracy)\n",
        "        if batch_precision != 'nan': precision.append(batch_precision)\n",
        "        if batch_recall != 'nan': recall.append(batch_recall)\n",
        "\n",
        "      print('\\n\\t - Train loss: {:.4f}'.format(loss / steps))\n",
        "      print('\\t - Validation Accuracy: {:.4f}'.format(sum(accuracy)/len(accuracy)))\n",
        "      print('\\t - Validation Precision: {:.4f}'.format(sum(precision)/len(precision)) if len(precision)>0 else '\\t - Validation Precision: NaN')\n",
        "      print('\\t - Validation Recall: {:.4f}'.format(sum(recall)/len(recall)) if len(recall)>0 else '\\t - Validation Recall: NaN')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute"
      ],
      "metadata": {
        "id": "81-vCN0fHKs4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data"
      ],
      "metadata": {
        "id": "wQewkWY2hjsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids_train, attention_mask_train, y_tf_train, input_ids_test, attention_mask_test, y_tf_test = tokenize(X_train, X_test, 32)\n",
        "train_dl, test_dl = data_load(input_ids_train, attention_mask_train, y_tf_train, input_ids_test, attention_mask_test, y_tf_test)"
      ],
      "metadata": {
        "id": "q0-IFfT3hjPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models"
      ],
      "metadata": {
        "id": "y66W_zLOgf8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\n",
        "    {\n",
        "        'learning_rate' : 5e-5,\n",
        "        'epochs' : 2\n",
        "    },\n",
        "    {\n",
        "        'learning_rate' : 5e-5,\n",
        "        'epochs' : 3\n",
        "    },\n",
        "    {\n",
        "        'learning_rate' : 2e-5,\n",
        "        'epochs' : 2\n",
        "    },\n",
        "    {\n",
        "        'learning_rate' : 2e-5,\n",
        "        'epochs' : 3\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "72RLxB8cgqS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execute Per Model"
      ],
      "metadata": {
        "id": "pNqSOdt6hDhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Learning rate : {models[0]['learning_rate']}\")\n",
        "print(f\"Epochs : {models[0]['epochs']}\")\n",
        "bert = bert_model(models[0]['learning_rate'], models[0]['epochs'])\n",
        "bert.train(train_dl, test_dl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r54TmhbCn0-Q",
        "outputId": "e86445b9-5a64-4df3-b072-66a02bca06f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate : 5e-05\n",
            "Epochs : 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch:  50%|█████     | 1/2 [02:15<02:15, 135.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.3778\n",
            "\t - Validation Accuracy: 0.8548\n",
            "\t - Validation Precision: 0.6671\n",
            "\t - Validation Recall: 0.7281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 2/2 [04:40<00:00, 140.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.2522\n",
            "\t - Validation Accuracy: 0.8604\n",
            "\t - Validation Precision: 0.7602\n",
            "\t - Validation Recall: 0.5654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Learning rate : {models[1]['learning_rate']}\")\n",
        "print(f\"Epochs : {models[1]['epochs']}\")\n",
        "bert = bert_model(models[1]['learning_rate'], models[1]['epochs'])\n",
        "bert.train(train_dl, test_dl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zTon2zKn3j1",
        "outputId": "a28ec7d7-b721-43dd-c9ad-23369178bf99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate : 5e-05\n",
            "Epochs : 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch:  33%|███▎      | 1/3 [02:27<04:54, 147.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.4212\n",
            "\t - Validation Accuracy: 0.7457\n",
            "\t - Validation Precision: NaN\n",
            "\t - Validation Recall: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  67%|██████▋   | 2/3 [04:54<02:27, 147.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.3178\n",
            "\t - Validation Accuracy: 0.8434\n",
            "\t - Validation Precision: 0.6306\n",
            "\t - Validation Recall: 0.7502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 3/3 [07:22<00:00, 147.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.2481\n",
            "\t - Validation Accuracy: 0.8391\n",
            "\t - Validation Precision: 0.6200\n",
            "\t - Validation Recall: 0.7697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Learning rate : {models[2]['learning_rate']}\")\n",
        "print(f\"Epochs : {models[2]['epochs']}\")\n",
        "bert = bert_model(models[2]['learning_rate'], models[2]['epochs'])\n",
        "bert.train(train_dl, test_dl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFkkM1PIn4VQ",
        "outputId": "3bc329c2-1a52-4ce0-e86c-50c9a8265a75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate : 2e-05\n",
            "Epochs : 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch:  50%|█████     | 1/2 [02:27<02:27, 147.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.3907\n",
            "\t - Validation Accuracy: 0.8587\n",
            "\t - Validation Precision: 0.7410\n",
            "\t - Validation Recall: 0.6126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 2/2 [04:55<00:00, 147.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.2701\n",
            "\t - Validation Accuracy: 0.8548\n",
            "\t - Validation Precision: 0.6916\n",
            "\t - Validation Recall: 0.6698\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Learning rate : {models[3]['learning_rate']}\")\n",
        "print(f\"Epochs : {models[3]['epochs']}\")\n",
        "bert = bert_model(models[3]['learning_rate'], models[3]['epochs'])\n",
        "bert.train(train_dl, test_dl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g897AT7KoGWH",
        "outputId": "f96b2cf6-b505-46f9-b426-51b342777a91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate : 2e-05\n",
            "Epochs : 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch:  33%|███▎      | 1/3 [02:27<04:55, 147.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.3892\n",
            "\t - Validation Accuracy: 0.8097\n",
            "\t - Validation Precision: 0.5622\n",
            "\t - Validation Recall: 0.8274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  67%|██████▋   | 2/3 [04:55<02:27, 147.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.2735\n",
            "\t - Validation Accuracy: 0.8622\n",
            "\t - Validation Precision: 0.7099\n",
            "\t - Validation Recall: 0.6526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 3/3 [07:23<00:00, 147.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.2002\n",
            "\t - Validation Accuracy: 0.8157\n",
            "\t - Validation Precision: 0.5699\n",
            "\t - Validation Recall: 0.8265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "gAy71fiH4h6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dari keempat model eksperimen, dapat disimpulkan bahwa model eksperimen terbaik adalah model dengan learning rate 5 x 10^-5 dan epochs sebanyak 3, yaitu dengan hasil metrics evaluasi :\n",
        "* Accuracy = 0.8391\n",
        "* Precision = 0.6200\n",
        "* Recall = 0.7697\n"
      ],
      "metadata": {
        "id": "LGOdAWA94lYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References"
      ],
      "metadata": {
        "id": "UKboexdUO7Mu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/fine-tuning-bert-for-text-classification-54e7df642894"
      ],
      "metadata": {
        "id": "3bRt8q1SO8sw"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Sxw6aqBp0Pjm",
        "qzEd5dMY4GRl",
        "8cIXJVpb46tz",
        "PMMiWHcEII3o",
        "Qex39bX7eJxv"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}